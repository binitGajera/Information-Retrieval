# Part 5 of the Project

In our final phase of the project, we will be exploring the idea of document clustering.

## Methodology

A similarity matrix is defined as a matrix in which entry i,j is the similarity of documents i and j, computed using the cosine score or some other metric. A document is perfectly similar to itself, so the entries on the main diagonal are all 1. Similarity is also symmetric, i.e. sim(i,j) = sim(j,i), so the similarity matrix is upper triangular in form. In this phase, we'll need to construct a similarity matrix.

Certain clustering algorithms use a similarity matrix to do their work. In hierarchical agglomerative clustering, two objects are merged if their centroids are closer to each other than are the centroids of any other pair of objects. An object could be a single document, or an existing cluster. Note that a document not yet in any cluster is in fact a trivial singleton cluster, with itself as the centroid.

## Code

I have used the variable weight_dict which is kind of similar to the term-document matrix and was generated in Phase 2 of the project. The first level has all the file names as keys and the inner level has the tokens with BM25 weight score as the value corresponding to each outer file. It is a nested dictionary. The sim_matrix which is the similarity matrix also is a nested dictionary and follows the same format as the weight_dict , but the inner level has other documents as keys and similarity score of that document with outer level document key as value. As new clusters are constructed, each cluster will represent a row in the similarity matrix. And the sim_matrix has been constructed in such manner that, as specified, it will be only a upper triangular matrix.

Along with the matrix, two other dictionaries current_clusters and cluster_info have been generated to keep track of the current clusters and to keep track of the number of documents in each cluster. Basically, after each cluster is generated by merging the documents, the merged documents/clusters are removed from being current whose track is being kept by current_clusters.

We have calculated the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to build the similarity matrix. Along with that, I have used [Group Average Link](https://nlp.stanford.edu/IR-book/html/htmledition/group-average-agglomerative-clustering-1.html) method to calculate the similarity score between two clusters or if one from the pair is cluster.

The file cluster.txt has top 100 lines of the original clusters that have been formed. The program continues the execution till and only if the calculated similarity score is greater than 0.4.

## Output

The output of the program is the centroid of the collection along with the running time of the entire program. We also generate a cluster.txt file in the main directory which would contain the top 100 lines, as per the requirement, of the clusters that have been generated. If the file does not have the .html extension at the end of its name then that file is a new generated cluster from the input files.
